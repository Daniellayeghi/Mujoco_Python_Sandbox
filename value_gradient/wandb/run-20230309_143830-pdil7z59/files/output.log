
Creating window glfw
Epochs: 0, Loss: 3270786304.0, lr: 0.004
Epochs: 1, Loss: 3195793664.0, lr: 0.004
Epochs: 2, Loss: 3106777856.0, lr: 0.004
Epochs: 3, Loss: 3030642688.0, lr: 0.004
Epochs: 4, Loss: 2971524864.0, lr: 0.004
Epochs: 5, Loss: 2912606208.0, lr: 0.004
Epochs: 6, Loss: 2853670400.0, lr: 0.004
Epochs: 7, Loss: 2800596736.0, lr: 0.004
Epochs: 8, Loss: 2754004224.0, lr: 0.004
Epochs: 9, Loss: 2710274304.0, lr: 0.004
Epochs: 10, Loss: 2667241216.0, lr: 0.004
Epochs: 11, Loss: 2625813248.0, lr: 0.004
Epochs: 12, Loss: 2587403520.0, lr: 0.004
Epochs: 13, Loss: 2551771648.0, lr: 0.004
Epochs: 14, Loss: 2517419520.0, lr: 0.004
Epochs: 15, Loss: 2483352320.0, lr: 0.004
Epochs: 16, Loss: 2449778688.0, lr: 0.004
Epochs: 17, Loss: 2417255936.0, lr: 0.004
Epochs: 18, Loss: 2385691904.0, lr: 0.004
Epochs: 19, Loss: 2354326016.0, lr: 0.004
Epochs: 20, Loss: 2322470400.0, lr: 0.004
Epochs: 21, Loss: 2289950464.0, lr: 0.004
Epochs: 22, Loss: 2256803840.0, lr: 0.004
Epochs: 23, Loss: 2222790144.0, lr: 0.004
Epochs: 24, Loss: 2187362304.0, lr: 0.004
Epochs: 25, Loss: 2150063616.0, lr: 0.004
Epochs: 26, Loss: 2110770304.0, lr: 0.004
Epochs: 27, Loss: 2069536512.0, lr: 0.004
Epochs: 28, Loss: 2026364928.0, lr: 0.004
Epochs: 29, Loss: 1981277312.0, lr: 0.004
Epochs: 30, Loss: 1934623232.0, lr: 0.004
Epochs: 31, Loss: 1887259136.0, lr: 0.004
Epochs: 32, Loss: 1840374912.0, lr: 0.004
Epochs: 33, Loss: 1795141632.0, lr: 0.004
Epochs: 34, Loss: 1752473472.0, lr: 0.004
Epochs: 35, Loss: 1712990208.0, lr: 0.004
Epochs: 36, Loss: 1676975616.0, lr: 0.004
Epochs: 37, Loss: 1644268672.0, lr: 0.004
Epochs: 38, Loss: 1614294912.0, lr: 0.004
Epochs: 39, Loss: 1586334592.0, lr: 0.004
Epochs: 40, Loss: 1559785344.0, lr: 0.004
Epochs: 41, Loss: 1534205824.0, lr: 0.004
Epochs: 42, Loss: 1509247232.0, lr: 0.004
Epochs: 43, Loss: 1484656000.0, lr: 0.004
Epochs: 44, Loss: 1460287872.0, lr: 0.004
Epochs: 45, Loss: 1436026368.0, lr: 0.004
Epochs: 46, Loss: 1411689984.0, lr: 0.004
Epochs: 47, Loss: 1387038720.0, lr: 0.004
Epochs: 48, Loss: 1361817984.0, lr: 0.004
Epochs: 49, Loss: 1335693056.0, lr: 0.004
Epochs: 50, Loss: 1308082688.0, lr: 0.004
Epochs: 51, Loss: 1278041472.0, lr: 0.004
Epochs: 52, Loss: 1244219648.0, lr: 0.004
Epochs: 53, Loss: 1204869760.0, lr: 0.004
Epochs: 54, Loss: 1158198400.0, lr: 0.004
Epochs: 55, Loss: 1103566208.0, lr: 0.004
Epochs: 56, Loss: 1043132992.0, lr: 0.004
Epochs: 57, Loss: 982891392.0, lr: 0.004
Epochs: 58, Loss: 935237632.0, lr: 0.004
Epochs: 59, Loss: 916655552.0, lr: 0.004
Epochs: 60, Loss: 927779968.0, lr: 0.004
Epochs: 61, Loss: 947392576.0, lr: 0.004
Epochs: 62, Loss: 953605888.0, lr: 0.004
Epochs: 63, Loss: 937729600.0, lr: 0.004
Epochs: 64, Loss: 907543744.0, lr: 0.004
Epochs: 65, Loss: 871679808.0, lr: 0.004
Epochs: 66, Loss: 837923136.0, lr: 0.004
Epochs: 67, Loss: 816735488.0, lr: 0.004
Epochs: 68, Loss: 811383232.0, lr: 0.004
Epochs: 69, Loss: 809562880.0, lr: 0.004
Epochs: 70, Loss: 805744640.0, lr: 0.004
Epochs: 71, Loss: 799371392.0, lr: 0.004
Epochs: 72, Loss: 789473536.0, lr: 0.004
Epochs: 73, Loss: 775768576.0, lr: 0.004
Epochs: 74, Loss: 759086208.0, lr: 0.004
Epochs: 75, Loss: 741262336.0, lr: 0.004
Epochs: 76, Loss: 724454336.0, lr: 0.004
Epochs: 77, Loss: 710640448.0, lr: 0.004
Epochs: 78, Loss: 701505536.0, lr: 0.004
Epochs: 79, Loss: 697995392.0, lr: 0.004
Epochs: 80, Loss: 697017792.0, lr: 0.004
Epochs: 81, Loss: 691471616.0, lr: 0.004
Epochs: 82, Loss: 678784128.0, lr: 0.004
Epochs: 83, Loss: 661436416.0, lr: 0.004
Epochs: 84, Loss: 643638464.0, lr: 0.004
Epochs: 85, Loss: 628530944.0, lr: 0.004
Epochs: 86, Loss: 617054336.0, lr: 0.004
Epochs: 87, Loss: 608256128.0, lr: 0.004
Epochs: 88, Loss: 600622080.0, lr: 0.004
Epochs: 89, Loss: 592934144.0, lr: 0.004
Epochs: 90, Loss: 584722176.0, lr: 0.004
Epochs: 91, Loss: 576074560.0, lr: 0.004
Epochs: 92, Loss: 567377984.0, lr: 0.004
Epochs: 93, Loss: 559445440.0, lr: 0.004
Epochs: 94, Loss: 552950208.0, lr: 0.004
Epochs: 95, Loss: 547858240.0, lr: 0.004
Epochs: 96, Loss: 543124608.0, lr: 0.004
Epochs: 97, Loss: 537172096.0, lr: 0.004
Epochs: 98, Loss: 529272480.0, lr: 0.004
Epochs: 99, Loss: 520380256.0, lr: 0.004
Epochs: 100, Loss: 512199552.0, lr: 0.004
Epochs: 101, Loss: 505608576.0, lr: 0.004
Epochs: 102, Loss: 500240576.0, lr: 0.004
Epochs: 103, Loss: 495193120.0, lr: 0.004
Epochs: 104, Loss: 489811264.0, lr: 0.004
Epochs: 105, Loss: 483982336.0, lr: 0.004
Epochs: 106, Loss: 478053376.0, lr: 0.004
Epochs: 107, Loss: 472555936.0, lr: 0.004
Epochs: 108, Loss: 467832832.0, lr: 0.004
Epochs: 109, Loss: 463733696.0, lr: 0.004
Epochs: 110, Loss: 459685056.0, lr: 0.004
Epochs: 111, Loss: 455200352.0, lr: 0.004
Epochs: 112, Loss: 450321120.0, lr: 0.004
Epochs: 113, Loss: 445462560.0, lr: 0.004
Epochs: 114, Loss: 440938816.0, lr: 0.004
Epochs: 115, Loss: 436777056.0, lr: 0.004
Epochs: 116, Loss: 432825184.0, lr: 0.004
Epochs: 117, Loss: 428909056.0, lr: 0.004
Epochs: 118, Loss: 424926592.0, lr: 0.004
Epochs: 119, Loss: 420884608.0, lr: 0.004
Epochs: 120, Loss: 416870336.0, lr: 0.004
Epochs: 121, Loss: 412973248.0, lr: 0.004
Epochs: 122, Loss: 409189376.0, lr: 0.004
Epochs: 123, Loss: 405355968.0, lr: 0.004
Epochs: 124, Loss: 401225696.0, lr: 0.004
Epochs: 125, Loss: 396688704.0, lr: 0.004
Epochs: 126, Loss: 391885696.0, lr: 0.004
Epochs: 127, Loss: 387389504.0, lr: 0.004
Epochs: 128, Loss: 385173472.0, lr: 0.004
Epochs: 129, Loss: 386800608.0, lr: 0.004
Epochs: 130, Loss: 383969312.0, lr: 0.004
Epochs: 131, Loss: 378924352.0, lr: 0.004
Epochs: 132, Loss: 374314528.0, lr: 0.004
Epochs: 133, Loss: 369618688.0, lr: 0.004
Epochs: 134, Loss: 365022368.0, lr: 0.004
Epochs: 135, Loss: 358713696.0, lr: 0.004
Epochs: 136, Loss: 354762624.0, lr: 0.004
Epochs: 137, Loss: 350590912.0, lr: 0.004
Epochs: 138, Loss: 346037024.0, lr: 0.004
Epochs: 139, Loss: 341908800.0, lr: 0.004
Epochs: 140, Loss: 336399840.0, lr: 0.004
Epochs: 141, Loss: 330320384.0, lr: 0.004
Epochs: 142, Loss: 324783552.0, lr: 0.004
Epochs: 143, Loss: 318293824.0, lr: 0.004
Epochs: 144, Loss: 311261088.0, lr: 0.004
Epochs: 145, Loss: 309253024.0, lr: 0.004
Epochs: 146, Loss: 305367264.0, lr: 0.004
Epochs: 147, Loss: 300967584.0, lr: 0.004
Epochs: 148, Loss: 298556448.0, lr: 0.004
Epochs: 149, Loss: 292484256.0, lr: 0.004
Epochs: 150, Loss: 284583520.0, lr: 0.004
Epochs: 151, Loss: 281384032.0, lr: 0.004
Epochs: 152, Loss: 278649856.0, lr: 0.004
Epochs: 153, Loss: 274584928.0, lr: 0.004
Epochs: 154, Loss: 273489152.0, lr: 0.004
Epochs: 155, Loss: 271275904.0, lr: 0.004
Epochs: 156, Loss: 260677376.0, lr: 0.004
Epochs: 157, Loss: 249210352.0, lr: 0.004
Epochs: 158, Loss: 250740176.0, lr: 0.004
Epochs: 159, Loss: 254527488.0, lr: 0.004
Epochs: 160, Loss: 252259472.0, lr: 0.004
Epochs: 161, Loss: 252442960.0, lr: 0.004
Epochs: 162, Loss: 248194864.0, lr: 0.004
Epochs: 163, Loss: 248939440.0, lr: 0.004
Epochs: 164, Loss: 249494432.0, lr: 0.004
Epochs: 165, Loss: 231285664.0, lr: 0.004
Epochs: 166, Loss: 211926096.0, lr: 0.004
Epochs: 167, Loss: 220646016.0, lr: 0.004
Epochs: 168, Loss: 229597776.0, lr: 0.004
Epochs: 169, Loss: 219158720.0, lr: 0.004
Epochs: 170, Loss: 204228384.0, lr: 0.004
Epochs: 171, Loss: 204821936.0, lr: 0.004
Epochs: 172, Loss: 218273136.0, lr: 0.004
Epochs: 173, Loss: 213854208.0, lr: 0.004
Epochs: 174, Loss: 201840320.0, lr: 0.004
Epochs: 175, Loss: 189390592.0, lr: 0.004
Epochs: 176, Loss: 184061856.0, lr: 0.004
Epochs: 177, Loss: 184431136.0, lr: 0.004
Epochs: 178, Loss: 184089968.0, lr: 0.004
Epochs: 179, Loss: 186944480.0, lr: 0.004
Epochs: 180, Loss: 177886656.0, lr: 0.004
Epochs: 181, Loss: 164318944.0, lr: 0.004
Epochs: 182, Loss: 167598544.0, lr: 0.004
Epochs: 183, Loss: 170744320.0, lr: 0.004
Epochs: 184, Loss: 170778848.0, lr: 0.004
Epochs: 185, Loss: 163617040.0, lr: 0.004
Epochs: 186, Loss: 159410256.0, lr: 0.004
Epochs: 187, Loss: 156611824.0, lr: 0.004
Epochs: 188, Loss: 155049024.0, lr: 0.004
Epochs: 189, Loss: 158881456.0, lr: 0.004
Epochs: 190, Loss: 156859680.0, lr: 0.004
Epochs: 191, Loss: 155730640.0, lr: 0.004
Epochs: 192, Loss: 154080368.0, lr: 0.004
Epochs: 193, Loss: 164399232.0, lr: 0.004
Epochs: 194, Loss: 173325680.0, lr: 0.004
Epochs: 195, Loss: 172991856.0, lr: 0.004
Epochs: 196, Loss: 163968576.0, lr: 0.004
Epochs: 197, Loss: 148932224.0, lr: 0.004
Epochs: 198, Loss: 150509680.0, lr: 0.004
Epochs: 199, Loss: 154431712.0, lr: 0.004
Epochs: 200, Loss: 152856192.0, lr: 0.004
Epochs: 201, Loss: 148103440.0, lr: 0.004
Epochs: 202, Loss: 146676560.0, lr: 0.004
Epochs: 203, Loss: 144851120.0, lr: 0.004
Epochs: 204, Loss: 143234128.0, lr: 0.004
Epochs: 205, Loss: 146207408.0, lr: 0.004
Epochs: 206, Loss: 145847856.0, lr: 0.004
Epochs: 207, Loss: 145360096.0, lr: 0.004
Epochs: 208, Loss: 145007216.0, lr: 0.004
Epochs: 209, Loss: 145076048.0, lr: 0.004
Epochs: 210, Loss: 145868512.0, lr: 0.004
Epochs: 211, Loss: 147174592.0, lr: 0.004
Epochs: 212, Loss: 148685456.0, lr: 0.004
Epochs: 213, Loss: 150807440.0, lr: 0.004
Epochs: 214, Loss: 151206896.0, lr: 0.004
Epochs: 215, Loss: 150394416.0, lr: 0.004
Epochs: 216, Loss: 149533232.0, lr: 0.004
Epochs: 217, Loss: 148840016.0, lr: 0.004
Epochs: 218, Loss: 149044480.0, lr: 0.004
Epochs: 219, Loss: 149219664.0, lr: 0.004
Epochs: 220, Loss: 152542368.0, lr: 0.004
Epochs: 221, Loss: 154974016.0, lr: 0.004
Epochs: 222, Loss: 153483872.0, lr: 0.004
Epochs: 223, Loss: 149508784.0, lr: 0.004
Epochs: 224, Loss: 146927472.0, lr: 0.004
Epochs: 225, Loss: 145321760.0, lr: 0.004
Epochs: 226, Loss: 140272704.0, lr: 0.004
Epochs: 227, Loss: 134048312.0, lr: 0.004
Epochs: 228, Loss: 129701312.0, lr: 0.004
Epochs: 229, Loss: 123652448.0, lr: 0.004
Epochs: 230, Loss: 125303192.0, lr: 0.004
Epochs: 231, Loss: 126263848.0, lr: 0.004
Epochs: 232, Loss: 124781384.0, lr: 0.004
Epochs: 233, Loss: 123766480.0, lr: 0.004
Epochs: 234, Loss: 124571520.0, lr: 0.004
Epochs: 235, Loss: 127384568.0, lr: 0.004
Epochs: 236, Loss: 129978184.0, lr: 0.004
Epochs: 237, Loss: 129544904.0, lr: 0.004
Epochs: 238, Loss: 126600400.0, lr: 0.004
Epochs: 239, Loss: 124348200.0, lr: 0.004
Epochs: 240, Loss: 124078688.0, lr: 0.004
Epochs: 241, Loss: 124351752.0, lr: 0.004
Epochs: 242, Loss: 124671128.0, lr: 0.004
Epochs: 243, Loss: 124839600.0, lr: 0.004
Epochs: 244, Loss: 124701344.0, lr: 0.004
Epochs: 245, Loss: 124409624.0, lr: 0.004
Epochs: 246, Loss: 123677928.0, lr: 0.004
Epochs: 247, Loss: 122670648.0, lr: 0.004
Epochs: 248, Loss: 122189096.0, lr: 0.004
Epochs: 249, Loss: 121828568.0, lr: 0.004
Epochs: 250, Loss: 121052696.0, lr: 0.001
Epochs: 251, Loss: 120046304.0, lr: 0.001
Epochs: 252, Loss: 119907208.0, lr: 0.001
Epochs: 253, Loss: 119873384.0, lr: 0.001
Epochs: 254, Loss: 119916400.0, lr: 0.001
Epochs: 255, Loss: 119988992.0, lr: 0.001
Epochs: 256, Loss: 120051128.0, lr: 0.001
Epochs: 257, Loss: 120081136.0, lr: 0.001
Epochs: 258, Loss: 120071312.0, lr: 0.001
Epochs: 259, Loss: 120025824.0, lr: 0.001
Epochs: 260, Loss: 119948416.0, lr: 0.001
Epochs: 261, Loss: 119847432.0, lr: 0.001
Epochs: 262, Loss: 119734760.0, lr: 0.001
Epochs: 263, Loss: 119625752.0, lr: 0.001
Epochs: 264, Loss: 119539552.0, lr: 0.001
Epochs: 265, Loss: 119493960.0, lr: 0.001
Epochs: 266, Loss: 119496256.0, lr: 0.001
Epochs: 267, Loss: 119542856.0, lr: 0.001
Epochs: 268, Loss: 119617992.0, lr: 0.001
Epochs: 269, Loss: 119701976.0, lr: 0.001
Epochs: 270, Loss: 119774136.0, lr: 0.001
Epochs: 271, Loss: 119820328.0, lr: 0.001
Epochs: 272, Loss: 119836248.0, lr: 0.001
Epochs: 273, Loss: 119821824.0, lr: 0.001
Epochs: 274, Loss: 119783768.0, lr: 0.001
Epochs: 275, Loss: 119732392.0, lr: 0.001
Epochs: 276, Loss: 119679176.0, lr: 0.001
Epochs: 277, Loss: 119635960.0, lr: 0.001
Epochs: 278, Loss: 119610768.0, lr: 0.001
Epochs: 279, Loss: 119607912.0, lr: 0.001
Epochs: 280, Loss: 119626104.0, lr: 0.001
Epochs: 281, Loss: 119658584.0, lr: 0.001
Epochs: 282, Loss: 119695544.0, lr: 0.001
Epochs: 283, Loss: 119726904.0, lr: 0.001
Epochs: 284, Loss: 119743872.0, lr: 0.001
Epochs: 285, Loss: 119742856.0, lr: 0.001
Epochs: 286, Loss: 119722064.0, lr: 0.001
Epochs: 287, Loss: 119685288.0, lr: 0.001
Epochs: 288, Loss: 119638424.0, lr: 0.001
Epochs: 289, Loss: 119588536.0, lr: 0.001
Epochs: 290, Loss: 119541344.0, lr: 0.001
Epochs: 291, Loss: 119502248.0, lr: 0.001
Epochs: 292, Loss: 119474264.0, lr: 0.001
Epochs: 293, Loss: 119456472.0, lr: 0.001
Epochs: 294, Loss: 119446544.0, lr: 0.001
Epochs: 295, Loss: 119440384.0, lr: 0.001
Epochs: 296, Loss: 119432744.0, lr: 0.001
Epochs: 297, Loss: 119419216.0, lr: 0.001
Epochs: 298, Loss: 119396072.0, lr: 0.001
Epochs: 299, Loss: 119363192.0, lr: 0.001
Epochs: 300, Loss: 119321152.0, lr: 0.001
Epochs: 301, Loss: 119271760.0, lr: 0.001
Epochs: 302, Loss: 119219288.0, lr: 0.001
Epochs: 303, Loss: 119166992.0, lr: 0.001
Epochs: 304, Loss: 119118016.0, lr: 0.001
Epochs: 305, Loss: 119074496.0, lr: 0.001
Epochs: 306, Loss: 119037232.0, lr: 0.001
Epochs: 307, Loss: 119005688.0, lr: 0.001
Epochs: 308, Loss: 118977248.0, lr: 0.001
Epochs: 309, Loss: 118950128.0, lr: 0.001
Epochs: 310, Loss: 118921272.0, lr: 0.001
Epochs: 311, Loss: 118889080.0, lr: 0.001
Epochs: 312, Loss: 118852184.0, lr: 0.001
Epochs: 313, Loss: 118810152.0, lr: 0.001
Epochs: 314, Loss: 118764280.0, lr: 0.001
Epochs: 315, Loss: 118715608.0, lr: 0.001
Epochs: 316, Loss: 118666856.0, lr: 0.001
Epochs: 317, Loss: 118618856.0, lr: 0.001
Epochs: 318, Loss: 118573696.0, lr: 0.001
Epochs: 319, Loss: 118532544.0, lr: 0.001
Epochs: 320, Loss: 118493960.0, lr: 0.001
Epochs: 321, Loss: 118458360.0, lr: 0.001
Epochs: 322, Loss: 118424168.0, lr: 0.001
Epochs: 323, Loss: 118389832.0, lr: 0.001
Epochs: 324, Loss: 118354688.0, lr: 0.001
Epochs: 325, Loss: 118316832.0, lr: 0.001
Epochs: 326, Loss: 118276960.0, lr: 0.001
Epochs: 327, Loss: 118234312.0, lr: 0.001
Epochs: 328, Loss: 118190384.0, lr: 0.001
Epochs: 329, Loss: 118144832.0, lr: 0.001
Epochs: 330, Loss: 118099984.0, lr: 0.001
Epochs: 331, Loss: 118055512.0, lr: 0.001
Epochs: 332, Loss: 118012568.0, lr: 0.001
Epochs: 333, Loss: 117970944.0, lr: 0.001
Epochs: 334, Loss: 117930896.0, lr: 0.001
Epochs: 335, Loss: 117891224.0, lr: 0.001
Epochs: 336, Loss: 117851952.0, lr: 0.001
Epochs: 337, Loss: 117811672.0, lr: 0.001
Epochs: 338, Loss: 117770488.0, lr: 0.001
Epochs: 339, Loss: 117727152.0, lr: 0.001
Epochs: 340, Loss: 117682696.0, lr: 0.001
Epochs: 341, Loss: 117637216.0, lr: 0.001
Epochs: 342, Loss: 117590352.0, lr: 0.001
Epochs: 343, Loss: 117542808.0, lr: 0.001
Epochs: 344, Loss: 117496032.0, lr: 0.001
Epochs: 345, Loss: 117448704.0, lr: 0.001
Epochs: 346, Loss: 117401904.0, lr: 0.001
Epochs: 347, Loss: 117356168.0, lr: 0.001
Epochs: 348, Loss: 117309640.0, lr: 0.001
Epochs: 349, Loss: 117263520.0, lr: 0.001
Epochs: 350, Loss: 117216656.0, lr: 0.001
Epochs: 351, Loss: 117169440.0, lr: 0.001
Epochs: 352, Loss: 117121264.0, lr: 0.001
Traceback (most recent call last):
  File "tv_value_synthesis_trajopt_cartpole.py", line 180, in <module>
    loss.backward()
  File "/home/daniel/.local/share/virtualenvs/Mujoco_Python_Sandbox-gNKKWzd6/lib/python3.7/site-packages/torch/_tensor.py", line 488, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/daniel/.local/share/virtualenvs/Mujoco_Python_Sandbox-gNKKWzd6/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "tv_value_synthesis_trajopt_cartpole.py", line 218, in <module>
    model_scripted = torch.jit.script(dyn_system.value_func.to('cpu'))  # Export to TorchScript
  File "/home/daniel/.local/share/virtualenvs/Mujoco_Python_Sandbox-gNKKWzd6/lib/python3.7/site-packages/torch/jit/_script.py", line 1287, in script
    obj, torch.jit._recursive.infer_methods_to_compile
  File "/home/daniel/.local/share/virtualenvs/Mujoco_Python_Sandbox-gNKKWzd6/lib/python3.7/site-packages/torch/jit/_recursive.py", line 476, in create_script_module
    return create_script_module_impl(nn_module, concrete_type, stubs_fn)
  File "/home/daniel/.local/share/virtualenvs/Mujoco_Python_Sandbox-gNKKWzd6/lib/python3.7/site-packages/torch/jit/_recursive.py", line 542, in create_script_module_impl
    create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)
  File "/home/daniel/.local/share/virtualenvs/Mujoco_Python_Sandbox-gNKKWzd6/lib/python3.7/site-packages/torch/jit/_recursive.py", line 393, in create_methods_and_properties_from_stubs
    concrete_type._create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)
RuntimeError:
attribute lookup is not defined on python value of type 'SimulationParams':
  File "tv_value_synthesis_trajopt_cartpole.py", line 64
    def forward(self, t, x):
        time = torch.ones((sim_params.nsim, 1, 1)).to(device) * t
                           ~~~~~~~~~~~~~~~ <--- HERE
        aug_x = torch.cat((x, time), dim=2)
        return self.nn(aug_x)
########## Saving Trace ##########